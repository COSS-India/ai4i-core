# LLM Service - Public API (as exposed via API Gateway)
openapi: 3.0.3
info:
  title: LLM Service
  description: Large Language Model inference (chat, completion).
  version: 1.0.0
servers:
  - url: "http://localhost:8080"
    description: API Gateway
tags:
  - name: LLM
  - name: Health
paths:
  /api/v1/llm/health:
    get:
      tags: [Health]
      summary: Health check
      description: Check service health and dependencies (Redis, PostgreSQL, Triton).
      security: []
      responses:
        "200":
          description: Service and dependencies are healthy
          content:
            application/json:
              schema:
                type: object
                properties:
                  status: { type: string, example: "healthy" }
                  service: { type: string, example: "llm-service" }
                  version: { type: string }
                  redis: { type: string, description: "Redis status" }
                  postgres: { type: string, description: "PostgreSQL status" }
                  triton: { type: string, description: "Triton inference server status" }
                  timestamp: { type: number }
        "503":
          description: Service or a dependency is unhealthy
  /api/v1/llm/inference:
    post:
      tags: [LLM]
      summary: LLM inference
      security: [{ BearerAuth: [] }]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/LLMInferenceRequest"
      responses:
        "200":
          content:
            application/json:
              schema: { $ref: "#/components/schemas/LLMInferenceResponse" }
        "401": { description: Unauthorized }
components:
  securitySchemes:
    BearerAuth:
      type: http
      scheme: bearer
      bearerFormat: API Key
  schemas:
    LLMTextInput:
      type: object
      required: [source]
      properties:
        source: { type: string, description: Input text to process }
    LLMInferenceConfig:
      type: object
      properties:
        serviceId: { type: string, description: LLM service/model ID (optional; Smart Router will assign if omitted) }
        inputLanguage: { type: string, description: "Input language code (e.g. 'en', 'hi')" }
        outputLanguage: { type: string, description: Output language code }
    LLMInferenceRequest:
      type: object
      required: [input, config]
      properties:
        input:
          type: array
          minItems: 1
          items: { $ref: "#/components/schemas/LLMTextInput" }
        config: { $ref: "#/components/schemas/LLMInferenceConfig" }
        controlConfig: { type: object, description: Additional control parameters }
    LLMOutput:
      type: object
      properties:
        source: { type: string, description: Source text }
        target: { type: string, description: Processed text }
    LLMInferenceResponse:
      type: object
      properties:
        output:
          type: array
          items: { $ref: "#/components/schemas/LLMOutput" }
security:
  - BearerAuth: []

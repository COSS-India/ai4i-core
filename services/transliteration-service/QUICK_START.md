# Quick Start Guide - Model Management Integration

## TL;DR

The transliteration service now fetches service and model info from the **Model Management Service** instead of using hardcoded values.

## What Changed?

**Before**: Hardcoded service registry
```python
SERVICE_REGISTRY = {
    "ai4bharat/indicxlit": ("65.1.35.3:8200", "transliteration"),
}
```

**After**: Dynamic lookup from database via Model Management Service
```python
# Fetches from model management service
service_info = await model_management_client.get_service("ai4bharat/indicxlit")
endpoint = service_info.endpoint  # e.g., "13.200.133.97:8000"
model_name = service_info.triton_model  # e.g., "transliteration"
```

## Setup (5 minutes)

### 1. Update Environment Variables

Add to your `.env` file:
```bash
MODEL_MANAGEMENT_SERVICE_URL=http://model-management-service:8091
TRANSLITERATION_ENDPOINT_CACHE_TTL=300
```

### 2. Ensure Model Management Service is Running

```bash
# Check if running
docker-compose ps model-management-service

# If not running, start it
docker-compose up -d model-management-service
```

### 3. Restart Transliteration Service

```bash
docker-compose restart transliteration-service

# Check logs
docker-compose logs -f transliteration-service | grep "Model Management"
```

You should see:
```
INFO: Initializing Model Management Client with URL: http://model-management-service:8091
INFO: âœ“ Model Management Client initialized successfully
```

## Register a New Service (3 steps)

### Step 1: Register the Model

```bash
curl -X POST http://localhost:8091/services/admin/create/model \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "modelId": "my-transliteration-model",
    "name": "My Transliteration Model",
    "version": "1.0.0",
    "description": "Custom transliteration model",
    "task": {
      "type": "transliteration"
    },
    "languages": [
      {
        "sourceLanguage": "en",
        "targetLanguage": "hi"
      }
    ],
    "domain": ["general"],
    "license": "MIT",
    "inferenceEndPoint": {
      "modelName": "transliteration"
    }
  }'
```

### Step 2: Register the Service

```bash
curl -X POST http://localhost:8091/services/admin/create/service \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "serviceId": "my-transliteration-service",
    "name": "My Transliteration Service",
    "modelId": "my-transliteration-model",
    "serviceDescription": "Production transliteration service",
    "hardwareDescription": "GPU: Tesla V100",
    "endpoint": "triton-server:8000",
    "apiKey": "your-triton-api-key"
  }'
```

### Step 3: Use the Service

```bash
curl -X POST http://localhost:8090/api/v1/transliteration/inference \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "input": [{"source": "namaste"}],
    "config": {
      "serviceId": "my-transliteration-service",
      "language": {
        "sourceLanguage": "en",
        "targetLanguage": "hi"
      },
      "isSentence": false
    }
  }'
```

## How It Works (Simple)

```
1. Request comes in with serviceId: "my-service"
   â†“
2. Check cache (in-memory or Redis)
   â†“ (if cache miss)
3. Call Model Management Service API
   â†“
4. Get service info (endpoint, model name, etc.)
   â†“
5. Cache the result (5 minutes by default)
   â†“
6. Create Triton client with dynamic endpoint
   â†“
7. Run inference
```

## Troubleshooting

### Problem: "Service not found"

**Solution**: Register the service in model management (see "Register a New Service" above)

### Problem: "No authentication headers"

**Solution**: Include auth header in your request:
```bash
curl -H "Authorization: Bearer YOUR_TOKEN" ...
# or
curl -H "X-API-Key: YOUR_API_KEY" ...
```

### Problem: "Connection refused to model management service"

**Solution**: Check if model management service is running:
```bash
docker-compose ps model-management-service
docker-compose logs model-management-service
```

### Problem: Service always uses fallback

**Check logs**:
```bash
docker-compose logs transliteration-service | grep -i "fallback\|model management"
```

**Common causes**:
1. Model management service not running
2. Service not registered in database
3. Authentication failure

## Fallback Behavior

If model management service is unavailable, the service falls back to hardcoded values:

```python
FALLBACK_SERVICE_REGISTRY = {
    "ai4bharat/indicxlit": ("65.1.35.3:8200", "transliteration"),
    "ai4bharat-transliteration": ("65.1.35.3:8200", "transliteration"),
    "indicxlit": ("65.1.35.3:8200", "transliteration"),
    "default": ("65.1.35.3:8200", "transliteration")
}
```

This ensures **zero downtime** even if model management service is down.

## Check Integration Status

```bash
# Check if model management client is initialized
docker-compose logs transliteration-service | grep "Model Management Client"

# Check if services are being fetched dynamically
docker-compose logs transliteration-service | grep "Fetching service info"

# Check cache hits
docker-compose logs transliteration-service | grep "cache hit"

# Check fallback usage
docker-compose logs transliteration-service | grep -i fallback
```

## Performance

### Cache Hit Rate

With default settings (300s TTL):
- **First request**: Fetches from model management service (~10-50ms)
- **Subsequent requests**: Served from cache (~0.1ms)
- **After 5 minutes**: Cache expires, fetches again

### Load Reduction

- **Without cache**: 1000 req/s = 1000 calls to model management service
- **With cache**: 1000 req/s = ~0.003 calls to model management service
- **Reduction**: 99.9997% ðŸš€

## Configuration Options

```bash
# Model Management Service URL
MODEL_MANAGEMENT_SERVICE_URL=http://model-management-service:8091

# Cache TTL (in seconds)
# Higher = better performance, lower = fresher data
TRANSLITERATION_ENDPOINT_CACHE_TTL=300  # 5 minutes (default)

# Fallback Triton endpoint (used when model management unavailable)
TRITON_ENDPOINT=65.1.35.3:8200
TRITON_API_KEY=your_api_key
```

## Testing

### Test Dynamic Lookup

```bash
# 1. Register a test service (see "Register a New Service" above)

# 2. Send request with that service ID
curl -X POST http://localhost:8090/api/v1/transliteration/inference \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "input": [{"source": "test"}],
    "config": {
      "serviceId": "your-test-service-id",
      "language": {"sourceLanguage": "en", "targetLanguage": "hi"}
    }
  }'

# 3. Check logs to confirm dynamic lookup
docker-compose logs transliteration-service | tail -20
```

### Test Fallback

```bash
# 1. Stop model management service
docker-compose stop model-management-service

# 2. Send request with fallback service ID
curl -X POST http://localhost:8090/api/v1/transliteration/inference \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "input": [{"source": "test"}],
    "config": {
      "serviceId": "ai4bharat/indicxlit",
      "language": {"sourceLanguage": "en", "targetLanguage": "hi"}
    }
  }'

# 3. Check logs for fallback messages
docker-compose logs transliteration-service | grep -i fallback

# 4. Restart model management service
docker-compose start model-management-service
```

## API Endpoints

### List Services

```bash
# Get all transliteration services from model management
curl http://localhost:8091/services/details/list_services?task_type=transliteration \
  -H "Authorization: Bearer YOUR_TOKEN"
```

### Get Service Details

```bash
# Get details for a specific service
curl -X POST http://localhost:8091/services/details/view_service \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"serviceId": "my-service-id"}'
```

### List Models

```bash
# Get all transliteration models
curl http://localhost:8091/services/details/list_models?task_type=transliteration \
  -H "Authorization: Bearer YOUR_TOKEN"
```

## Common Use Cases

### Use Case 1: Add New Triton Server

1. Deploy new Triton server with transliteration model
2. Register model in model management
3. Register service pointing to new Triton server
4. Use service ID in transliteration requests
5. **No code changes needed!** âœ…

### Use Case 2: Update Triton Endpoint

1. Update service endpoint in model management:
   ```bash
   curl -X PATCH http://localhost:8091/services/admin/update/service \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -d '{
       "serviceId": "my-service",
       "endpoint": "new-triton-server:8000"
     }'
   ```
2. Wait for cache to expire (5 minutes) or restart service
3. **No code changes needed!** âœ…

### Use Case 3: A/B Testing Different Models

1. Register two services with different models:
   - `transliteration-model-v1` â†’ `service-v1`
   - `transliteration-model-v2` â†’ `service-v2`
2. Route traffic based on service ID:
   ```python
   service_id = "service-v1" if user.is_beta else "service-v2"
   ```
3. Compare results
4. **No infrastructure changes needed!** âœ…

## Documentation

- **Full Integration Guide**: `MODEL_MANAGEMENT_INTEGRATION.md`
- **Summary of Changes**: `INTEGRATION_SUMMARY.md`
- **This Quick Start**: `QUICK_START.md`

## Need Help?

1. Check logs: `docker-compose logs -f transliteration-service`
2. Check model management service: `docker-compose logs -f model-management-service`
3. Read troubleshooting guide: `MODEL_MANAGEMENT_INTEGRATION.md#troubleshooting`
4. Contact the AI4ICore team

## Summary

âœ… **Easy Setup**: Just add 2 environment variables  
âœ… **Zero Downtime**: Fallback support ensures continuity  
âœ… **High Performance**: Efficient caching (99.9997% load reduction)  
âœ… **Flexible**: Add/update services without code changes  
âœ… **Well Tested**: Same pattern as NMT service  

**You're all set!** ðŸŽ‰


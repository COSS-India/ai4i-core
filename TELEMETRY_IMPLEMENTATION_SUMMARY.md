# üìä Complete Telemetry Service Implementation Summary

## üéØ **WHAT: Overview of Implementation**

### **Purpose**
Implemented a comprehensive observability stack for the AI4ICore microservices platform, enabling:
- **Log Aggregation**: Centralized log collection, storage, and visualization
- **Distributed Tracing**: End-to-end request tracing across microservices
- **Metrics Collection**: Service performance and business metrics
- **Dashboard Visualization**: Unified view of system health and performance

### **Key Components Implemented**

1. **OpenSearch** - Log storage and search engine (replaces Elasticsearch)
2. **OpenSearch Dashboards** - Log visualization UI (replaces Kibana)
3. **Jaeger** - Distributed tracing backend
4. **Fluent Bit** - Log collection agent
5. **Telemetry Service** - Central telemetry processing service
6. **AI4ICore Telemetry Library** - Shared library for tracing integration
7. **Grafana Integration** - Jaeger datasource for unified dashboards

---

## üîç **WHY: Reasons for Implementation**

### **Business Drivers**
1. **Observability Gap**: No centralized way to monitor distributed microservices
2. **Debugging Challenges**: Difficult to trace requests across multiple services
3. **Performance Monitoring**: Need visibility into service performance and bottlenecks
4. **Compliance & Auditing**: Requirement for log retention and audit trails
5. **Cost Optimization**: OpenSearch is open-source alternative to Elasticsearch

### **Technical Benefits**
- **Distributed Tracing**: Track requests across service boundaries
- **Correlation**: Link logs, traces, and metrics using correlation IDs
- **Real-time Monitoring**: Immediate visibility into system health
- **Scalability**: Handle high-volume log ingestion
- **Searchability**: Fast search across historical logs

---

## üèóÔ∏è **HOW: Technical Implementation Details**

### **Architecture Flow**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MICROSERVICES LAYER                           ‚îÇ
‚îÇ  (NER, OCR, ASR, TTS, NMT, LLM, etc.)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ                                     ‚îÇ
                            ‚ñº                                     ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   LOGGING PATH               ‚îÇ      ‚îÇ   TRACING PATH                ‚îÇ
        ‚îÇ                              ‚îÇ      ‚îÇ                                ‚îÇ
        ‚îÇ  1. Structured JSON Logs     ‚îÇ      ‚îÇ  1. OpenTelemetry SDK         ‚îÇ
        ‚îÇ  2. Kafka Handler (optional) ‚îÇ      ‚îÇ  2. OTLP Exporter             ‚îÇ
        ‚îÇ  3. stdout (fallback)        ‚îÇ      ‚îÇ  3. BatchSpanProcessor        ‚îÇ
        ‚îÇ  4. Fluent Bit Collection    ‚îÇ      ‚îÇ  4. Jaeger Collector          ‚îÇ
        ‚îÇ  5. OpenSearch Storage      ‚îÇ      ‚îÇ  5. Jaeger UI                  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ                                     ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                              ‚ñº
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ   METRICS PATH               ‚îÇ
                            ‚îÇ                              ‚îÇ
                            ‚îÇ  1. Prometheus Exporter      ‚îÇ
                            ‚îÇ  2. /metrics endpoint         ‚îÇ
                            ‚îÇ  3. Prometheus Scraping      ‚îÇ
                            ‚îÇ  4. Grafana Visualization    ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ **COMPONENT DETAILS**

### **1. OpenSearch (Log Storage)**

**Location**: `docker-compose.yml` (lines 1000-1027)

**Configuration**:
- **Image**: `opensearchproject/opensearch:2.11.0`
- **Ports**: 
  - `9204:9200` (HTTP API)
  - `9600:9600` (Performance Analyzer)
- **Memory**: 512MB heap (`-Xms512m -Xmx512m`)
- **Security**: Disabled for local development
- **Volume**: `opensearch-data` for persistence

**Config File**: `infrastructure/opensearch/opensearch.yml`
```yaml
cluster.name: docker-cluster
network.host: 0.0.0.0
discovery.type: single-node
plugins.security.disabled: true
```

**Purpose**: 
- Store structured JSON logs from all services
- Enable fast full-text search
- Support log retention policies
- Provide REST API for log queries

---

### **2. OpenSearch Dashboards (Log Visualization)**

**Location**: `docker-compose.yml` (lines 1029-1043)

**Configuration**:
- **Image**: `opensearchproject/opensearch-dashboards:2.11.0`
- **Port**: `5602:5601` (Web UI)
- **Environment**: 
  - `OPENSEARCH_HOSTS=http://opensearch:9200`
  - `DISABLE_SECURITY_DASHBOARDS_PLUGIN=true`

**Access**: `http://localhost:5602`

**Features**:
- Create index patterns (e.g., `logs-*`)
- Build visualizations and dashboards
- Search and filter logs
- View log details with JSON formatting
- Time-based log analysis

---

### **3. Jaeger (Distributed Tracing)**

**Location**: `docker-compose.yml` (lines 1045-1059)

**Configuration**:
- **Image**: `jaegertracing/all-in-one:latest`
- **Ports**:
  - `16686:16686` - Web UI
  - `14268:14268` - HTTP collector
  - `6831:6831/udp` - UDP agent (Thrift)
  - `4317:4317` - OTLP gRPC (primary)
  - `4318:4318` - OTLP HTTP
- **Environment**: `COLLECTOR_OTLP_ENABLED=true`

**Access**: `http://localhost:16686`

**Purpose**:
- Collect distributed traces from all services
- Visualize request flows across services
- Identify performance bottlenecks
- Debug cross-service issues
- Analyze service dependencies

---

### **4. Fluent Bit (Log Collection)**

**Location**: `docker-compose.yml` (lines 1061-1074)

**Configuration**:
- **Image**: `fluent/fluent-bit:latest`
- **Volumes**:
  - `./infrastructure/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf`
  - `/var/lib/docker/containers:/var/lib/docker/containers:ro`
  - `/var/run/docker.sock:/var/run/docker.sock:ro`

**Config File**: `infrastructure/fluent-bit/fluent-bit.conf`

**Flow**:
1. **INPUT**: Tail Docker container logs from `/var/lib/docker/containers/*/*.log`
2. **FILTER**: Parse JSON logs, add metadata (collector, environment)
3. **OUTPUT**: Send to OpenSearch with Logstash format

**Key Settings**:
- Index: `logs`
- Logstash format: `logs-YYYY.MM.DD`
- Retry limit: 3
- Flush interval: 5 seconds

---

### **5. Telemetry Service**

**Location**: `services/telemetry-service/main.py`

**Purpose**: Central service for telemetry data processing and routing

**Connections**:
- **OpenSearch**: Primary log storage
- **Elasticsearch**: Legacy support (for other teams)
- **PostgreSQL**: Telemetry metadata storage
- **Redis**: Caching and rate limiting
- **Kafka**: Log streaming (producer/consumer)

**Endpoints**:
- `GET /` - Service info
- `GET /health` - Health check (checks all connections)
- `GET /api/v1/telemetry/status` - Status information
- `POST /api/v1/telemetry/logs` - Log ingestion (to be implemented)
- `POST /api/v1/telemetry/traces` - Trace ingestion (to be implemented)
- `GET /api/v1/telemetry/logs/search` - Log search (to be implemented)
- `GET /api/v1/telemetry/traces/search` - Trace search (to be implemented)

**Environment Variables**: `services/telemetry-service/env.template`
- `OPENSEARCH_URL=http://opensearch:9200`
- `OPENSEARCH_USERNAME=admin`
- `OPENSEARCH_PASSWORD=admin123`
- `JAEGER_ENDPOINT=http://jaeger:4317`

---

### **6. AI4ICore Telemetry Library**

**Location**: `libs/ai4icore_telemetry/`

**Structure**:
```
libs/ai4icore_telemetry/
‚îú‚îÄ‚îÄ ai4icore_telemetry/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # Exports setup_tracing, get_tracer
‚îÇ   ‚îî‚îÄ‚îÄ tracing.py            # OpenTelemetry setup logic
‚îú‚îÄ‚îÄ pyproject.toml            # Package dependencies
‚îî‚îÄ‚îÄ README.md                 # Usage documentation
```

**Key Function**: `setup_tracing(service_name, jaeger_endpoint)`

**Dependencies** (from `pyproject.toml`):
- `opentelemetry-api>=1.20.0`
- `opentelemetry-sdk>=1.20.0`
- `opentelemetry-instrumentation-fastapi>=0.41b0`
- `opentelemetry-exporter-otlp-proto-grpc>=1.20.0`
- `elasticsearch>=8.10.0` (OpenSearch compatible)

**How It Works**:
1. Creates OpenTelemetry `Resource` with service name
2. Sets up `TracerProvider`
3. Configures `OTLPSpanExporter` (preferred) or `JaegerExporter` (fallback)
4. Adds `BatchSpanProcessor` for efficient span batching
5. Returns tracer instance

**Exporter Priority**:
1. **OTLP gRPC** (recommended) - Uses `opensearch:4317`
2. **Jaeger Thrift** (fallback) - Uses UDP port 6831

---

### **7. Service Integration Pattern**

**Example**: NER Service (`services/ner-service/main.py`)

**Step 1: Import Libraries**
```python
from ai4icore_telemetry import setup_tracing
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from ai4icore_observability import ObservabilityPlugin, PluginConfig
from ai4icore_logging import get_logger, CorrelationMiddleware, configure_logging
```

**Step 2: Configure Logging**
```python
configure_logging(
    service_name=os.getenv("SERVICE_NAME", "ner-service"),
    use_kafka=os.getenv("USE_KAFKA_LOGGING", "false").lower() == "true",
)
logger = get_logger(__name__)
```

**Step 3: Setup Observability Plugin**
```python
config = PluginConfig.from_env()
config.enabled = True
config.apps = ["ner"]
plugin = ObservabilityPlugin(config)
plugin.register_plugin(app)
```

**Step 4: Setup Distributed Tracing**
```python
# IMPORTANT: Setup tracing BEFORE instrumenting FastAPI
tracer = setup_tracing("ner-service")
if tracer:
    logger.info("‚úÖ Distributed tracing initialized for NER service")
    FastAPIInstrumentor.instrument_app(app)
    logger.info("‚úÖ FastAPI instrumentation enabled for tracing")
```

**Step 5: Add Middleware (Order Matters!)**
```python
# 1. CORS (first)
app.add_middleware(CORSMiddleware, ...)

# 2. Correlation (extracts X-Correlation-ID)
app.add_middleware(CorrelationMiddleware)

# 3. Request Logging (uses correlation ID)
app.add_middleware(RequestLoggingMiddleware)

# 4. Rate Limiting
app.add_middleware(RateLimitMiddleware, ...)
```

---

### **8. Request Logging Middleware**

**Location**: `services/ner-service/middleware/request_logging.py`

**Features**:
- Captures request metadata (method, path, IP, user-agent)
- Extracts auth context (user_id, api_key_id)
- Gets correlation ID from headers
- Calculates processing time
- Logs with appropriate level (info/warning/error)
- Adds `X-Process-Time` header to response

**Log Structure**:
```json
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "level": "INFO",
  "service": "ner-service",
  "message": "GET /api/v1/ner/inference - 200 - 0.234s",
  "context": {
    "method": "GET",
    "path": "/api/v1/ner/inference",
    "status_code": 200,
    "duration_ms": 234.56,
    "client_ip": "192.168.1.100",
    "user_agent": "Mozilla/5.0...",
    "user_id": "user_123",
    "api_key_id": "key_456",
    "correlation_id": "corr-789"
  }
}
```

---

### **9. Grafana Integration**

**Location**: `infrastructure/grafana/provisioning/datasources/prometheus.yaml`

**Datasources Configured**:
1. **Prometheus** (default)
   - URL: `http://prometheus:9090`
   - Type: `prometheus`
   - Purpose: Metrics visualization

2. **Jaeger**
   - URL: `http://jaeger:16686`
   - Type: `jaeger`
   - Purpose: Trace visualization in Grafana

**Access**: `http://localhost:3001`

---

## üîÑ **FLOW OF EXECUTION**

### **Request Flow with Tracing**

```
1. Client Request
   ‚îî‚îÄ> API Gateway (Kong)
       ‚îî‚îÄ> Service (e.g., NER Service)
           ‚îÇ
           ‚îú‚îÄ> CorrelationMiddleware
           ‚îÇ   ‚îî‚îÄ> Extracts/generates X-Correlation-ID
           ‚îÇ
           ‚îú‚îÄ> FastAPIInstrumentor (OpenTelemetry)
           ‚îÇ   ‚îî‚îÄ> Creates root span
           ‚îÇ       ‚îî‚îÄ> Adds trace_id, span_id
           ‚îÇ
           ‚îú‚îÄ> RequestLoggingMiddleware
           ‚îÇ   ‚îî‚îÄ> Logs request with correlation_id
           ‚îÇ       ‚îî‚îÄ> JSON log ‚Üí stdout ‚Üí Fluent Bit ‚Üí OpenSearch
           ‚îÇ
           ‚îú‚îÄ> Business Logic
           ‚îÇ   ‚îî‚îÄ> Creates child spans for operations
           ‚îÇ       ‚îî‚îÄ> Database queries
           ‚îÇ       ‚îî‚îÄ> External API calls
           ‚îÇ       ‚îî‚îÄ> Model inference
           ‚îÇ
           ‚îú‚îÄ> Response
           ‚îÇ   ‚îî‚îÄ> Logs response with duration
           ‚îÇ
           ‚îî‚îÄ> Span Export
               ‚îî‚îÄ> BatchSpanProcessor
                   ‚îî‚îÄ> OTLPSpanExporter
                       ‚îî‚îÄ> Jaeger Collector (port 4317)
                           ‚îî‚îÄ> Jaeger Storage
                               ‚îî‚îÄ> Jaeger UI (port 16686)
```

### **Log Flow**

```
1. Service Logs Event
   ‚îî‚îÄ> ai4icore_logging.get_logger()
       ‚îî‚îÄ> JSONFormatter
           ‚îî‚îÄ> KafkaHandler (if enabled)
               ‚îî‚îÄ> Kafka Topic: "logs"
           ‚îî‚îÄ> OR stdout (fallback)
               ‚îî‚îÄ> Docker Container Log
                   ‚îî‚îÄ> /var/lib/docker/containers/*/logs
                       ‚îî‚îÄ> Fluent Bit (tail input)
                           ‚îî‚îÄ> JSON Parser
                               ‚îî‚îÄ> Metadata Enrichment
                                   ‚îî‚îÄ> OpenSearch Output
                                       ‚îî‚îÄ> Index: logs-YYYY.MM.DD
                                           ‚îî‚îÄ> OpenSearch Dashboards
```

### **Trace Flow**

```
1. Service Receives Request
   ‚îî‚îÄ> setup_tracing("service-name")
       ‚îî‚îÄ> TracerProvider initialized
           ‚îî‚îÄ> OTLPSpanExporter configured
               ‚îî‚îÄ> BatchSpanProcessor added
                   ‚îî‚îÄ> FastAPIInstrumentor.instrument_app(app)
                       ‚îî‚îÄ> Automatic span creation for:
                           ‚îú‚îÄ> HTTP requests
                           ‚îú‚îÄ> Database queries (if instrumented)
                           ‚îî‚îÄ> External calls (if instrumented)
                               ‚îî‚îÄ> Spans batched and exported
                                   ‚îî‚îÄ> Jaeger Collector (OTLP gRPC :4317)
                                       ‚îî‚îÄ> Jaeger Storage
                                           ‚îî‚îÄ> Queryable via Jaeger UI
```

---

## üìç **WHERE: File Locations**

### **Infrastructure**
- `docker-compose.yml` - Service definitions (lines 1000-1074)
- `infrastructure/opensearch/opensearch.yml` - OpenSearch config
- `infrastructure/fluent-bit/fluent-bit.conf` - Fluent Bit config
- `infrastructure/grafana/provisioning/datasources/prometheus.yaml` - Grafana datasources

### **Libraries**
- `libs/ai4icore_telemetry/` - Telemetry library
  - `ai4icore_telemetry/tracing.py` - Tracing setup
  - `ai4icore_telemetry/__init__.py` - Public API
  - `pyproject.toml` - Dependencies
- `libs/ai4icore_observability/` - Observability plugin
  - `plugin.py` - Main plugin class
  - `metrics.py` - Metrics collection
- `libs/ai4icore_logging/` - Logging library
  - `logger.py` - Logger setup
  - `formatters.py` - JSON formatter
  - `handlers.py` - Kafka handler

### **Services**
- `services/telemetry-service/` - Central telemetry service
  - `main.py` - Service implementation
  - `env.template` - Environment variables
  - `requirements.txt` - Dependencies
- `services/ner-service/` - Example integrated service
  - `main.py` - Service with tracing
  - `middleware/request_logging.py` - Request logging
- `services/ocr-service/` - Another integrated service

### **Documentation**
- `STEP_BY_STEP_IMPLEMENTATION.md` - Implementation guide
- `IMPLEMENTATION_GUIDE.md` - Detailed guide

---

## ‚è∞ **WHEN: Execution Timeline**

### **Service Startup Sequence**

1. **Infrastructure Services** (docker-compose)
   - PostgreSQL ‚Üí Redis ‚Üí Kafka ‚Üí OpenSearch ‚Üí Jaeger
   - OpenSearch Dashboards (waits for OpenSearch)
   - Fluent Bit (waits for OpenSearch)

2. **Application Services**
   - Each service initializes in this order:
     ```
     a. Configure logging (JSON formatter, Kafka handler)
     b. Initialize ObservabilityPlugin
     c. Setup tracing (setup_tracing)
     d. Instrument FastAPI (FastAPIInstrumentor)
     e. Register middleware (CORS, Correlation, Logging, RateLimit)
     f. Start FastAPI server
     ```

3. **Runtime**
   - Every request creates:
     - Log entry (via RequestLoggingMiddleware)
     - Trace span (via FastAPIInstrumentor)
     - Metrics (via ObservabilityMiddleware)

---

## üîë **KEY TERMS & CONCEPTS**

### **OpenTelemetry (OTel)**
- **What**: Open standard for observability
- **Components**:
  - **Tracer**: Creates spans
  - **Span**: Represents a unit of work
  - **Trace**: Collection of related spans
  - **Exporter**: Sends spans to backend (Jaeger)
  - **Resource**: Metadata about service

### **OTLP (OpenTelemetry Protocol)**
- **What**: Protocol for sending telemetry data
- **Formats**: gRPC (port 4317) or HTTP (port 4318)
- **Why**: Standardized way to send traces to any backend

### **Jaeger**
- **What**: Distributed tracing system
- **Components**:
  - **Agent**: Receives spans (UDP 6831)
  - **Collector**: Receives spans (OTLP 4317)
  - **Query**: Retrieves traces (HTTP 16686)
  - **Storage**: In-memory (all-in-one) or external DB

### **OpenSearch**
- **What**: Open-source search and analytics engine
- **Compatible**: With Elasticsearch API
- **Indices**: `logs-YYYY.MM.DD` (daily rotation)
- **Query**: REST API or OpenSearch Dashboards

### **Fluent Bit**
- **What**: Lightweight log processor
- **Input**: Docker container logs
- **Filter**: Parse, enrich, transform
- **Output**: OpenSearch, Kafka, etc.

### **Correlation ID**
- **What**: Unique identifier for request
- **Header**: `X-Correlation-ID`
- **Purpose**: Link logs, traces, and metrics
- **Flow**: Propagated across service calls

### **Span**
- **What**: Single operation in a trace
- **Attributes**:
  - `trace_id`: Unique trace identifier
  - `span_id`: Unique span identifier
  - `parent_span_id`: Parent span (for hierarchy)
  - `operation_name`: Name of operation
  - `start_time`, `end_time`: Duration
  - `tags`: Key-value metadata
  - `logs`: Events within span

### **Trace**
- **What**: Complete request journey
- **Structure**: Tree of spans
- **Root Span**: Initial request
- **Child Spans**: Sub-operations

---

## üéØ **INTEGRATION CHECKLIST**

### **Services Integrated**
- ‚úÖ **NER Service** (`services/ner-service/main.py`)
- ‚úÖ **OCR Service** (`services/ocr-service/main.py`)
- ‚ö†Ô∏è **Other Services** - Can follow same pattern

### **Integration Steps for New Service**

1. **Add Dependencies** (Dockerfile)
   ```dockerfile
   COPY libs/ai4icore_telemetry /app/libs/ai4icore_telemetry
   RUN pip install --no-cache-dir --user -e /app/libs/ai4icore_telemetry
   ```

2. **Import Libraries** (main.py)
   ```python
   from ai4icore_telemetry import setup_tracing
   from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
   ```

3. **Setup Tracing** (main.py)
   ```python
   tracer = setup_tracing("service-name")
   if tracer:
       FastAPIInstrumentor.instrument_app(app)
   ```

4. **Add Middleware** (main.py)
   ```python
   app.add_middleware(CorrelationMiddleware)
   app.add_middleware(RequestLoggingMiddleware)
   ```

5. **Environment Variables** (.env)
   ```bash
   JAEGER_ENDPOINT=http://jaeger:4317
   SERVICE_NAME=service-name
   ```

---

## üîç **VERIFICATION & TESTING**

### **Check OpenSearch**
```bash
# Health check
curl http://localhost:9204/_cluster/health

# List indices
curl http://localhost:9204/_cat/indices?v

# Search logs
curl -X POST "http://localhost:9204/logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "query": { "match_all": {} },
  "size": 10
}'
```

### **Check OpenSearch Dashboards**
- Open: `http://localhost:5602`
- Create index pattern: `logs-*`
- View logs in Discover

### **Check Jaeger**
```bash
# List services
curl http://localhost:16686/api/services

# Get traces
curl http://localhost:16686/api/traces?service=ner-service
```

### **Check Jaeger UI**
- Open: `http://localhost:16686`
- Select service: `ner-service` or `ocr-service`
- View traces and spans

### **Check Grafana**
- Open: `http://localhost:3001`
- Login: `admin/admin`
- Verify datasources: Prometheus, Jaeger
- Create dashboards

### **Test Service Integration**
```bash
# Make request to service
curl -X POST http://localhost:9001/api/v1/ner/inference \
  -H "Content-Type: application/json" \
  -H "X-Correlation-ID: test-123" \
  -d '{"text": "test"}'

# Check logs in OpenSearch Dashboards
# Check trace in Jaeger UI
```

---

## üìä **DASHBOARD ACCESS**

### **OpenSearch Dashboards**
- **URL**: `http://localhost:5602`
- **Purpose**: Log visualization and search
- **Features**:
  - Create index patterns
  - Build visualizations
  - Search and filter logs
  - Time-based analysis

### **Jaeger UI**
- **URL**: `http://localhost:16686`
- **Purpose**: Distributed trace visualization
- **Features**:
  - Search traces by service
  - View trace timeline
  - Analyze span details
  - Service dependency graph

### **Grafana**
- **URL**: `http://localhost:3001`
- **Purpose**: Unified observability dashboard
- **Features**:
  - Metrics from Prometheus
  - Traces from Jaeger
  - Custom dashboards
  - Alerts

---

## üöÄ **NEXT STEPS & FUTURE ENHANCEMENTS**

### **To Be Implemented**
1. **Telemetry Service Endpoints**
   - Log ingestion API
   - Trace ingestion API
   - Search APIs
   - Retention policies

2. **Additional Service Integration**
   - ASR, TTS, NMT services
   - LLM service
   - Pipeline service
   - All other microservices

3. **Advanced Features**
   - Log correlation with traces
   - Anomaly detection
   - Alerting rules
   - Custom dashboards
   - Performance SLAs

4. **Production Hardening**
   - OpenSearch security (TLS, auth)
   - Jaeger persistent storage
   - Log retention policies
   - Backup strategies

---

## üìù **SUMMARY**

### **What Was Built**
- Complete observability stack with OpenSearch, Jaeger, and Prometheus
- Shared telemetry library for easy service integration
- Central telemetry service for data processing
- Log collection pipeline with Fluent Bit
- Distributed tracing with OpenTelemetry
- Unified dashboards in Grafana

### **Key Achievements**
- ‚úÖ Replaced Elasticsearch with OpenSearch
- ‚úÖ Added distributed tracing with Jaeger
- ‚úÖ Integrated multiple services (NER, OCR)
- ‚úÖ Created reusable telemetry library
- ‚úÖ Set up log aggregation pipeline
- ‚úÖ Configured visualization dashboards

### **Architecture Pattern**
```
Services ‚Üí Logs (OpenSearch) + Traces (Jaeger) + Metrics (Prometheus)
         ‚Üì
    Unified Dashboards (Grafana, OpenSearch Dashboards, Jaeger UI)
```

---

**Document Version**: 1.0  
**Last Updated**: 2024  
**Maintained By**: AI4ICore Team

version: '3.8'

services:
  # Infrastructure Services
  postgres:
    image: postgres:15-alpine
    container_name: ai4v-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER} 
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5434:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./infrastructure/postgres:/docker-entrypoint-initdb.d
    command: >
      postgres
      -c password_encryption=md5
      -c listen_addresses='*'
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $POSTGRES_USER -d $POSTGRES_DB -h localhost -p 5432"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: ai4v-redis
    command: redis-server /usr/local/etc/redis/redis.conf
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    ports:
      - "6381:6379"
    volumes:
      - redis-data:/data
      - ./infrastructure/redis/redis.conf:/usr/local/etc/redis/redis.conf
    networks:
      microservices-network:
        aliases:
          - redis
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -h localhost -p 6379 -a \"$REDIS_PASSWORD\" ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  influxdb:
    image: influxdb:2.7-alpine
    container_name: ai4v-influxdb
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: ${INFLUXDB_ADMIN_USER}
      DOCKER_INFLUXDB_INIT_PASSWORD: ${INFLUXDB_ADMIN_PASSWORD}
      DOCKER_INFLUXDB_INIT_ORG: ${INFLUXDB_ORG}
      DOCKER_INFLUXDB_INIT_BUCKET: ${INFLUXDB_BUCKET}
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: ${INFLUXDB_ADMIN_TOKEN}
    ports:
      - "8089:8086"
    volumes:
      - influxdb-data:/var/lib/influxdb2
      - ./infrastructure/influxdb:/docker-entrypoint-initdb.d
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:8086/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
    container_name: ai4v-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTIC_USERNAME=${ELASTIC_USERNAME}
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9203:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
      - ./infrastructure/elasticsearch:/docker-entrypoint-initdb.d
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD-SHELL", "curl -u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD} -f http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=5s || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: ai4v-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped

  unleash:
    image: unleashorg/unleash-server:latest
    container_name: ai4v-unleash-server
    ports:
      - "4242:4242"
    environment:
      DATABASE_URL: postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/unleash
      DATABASE_SSL: "false"
      UNLEASH_URL: http://unleash:4242
      LOG_LEVEL: info
      BASE_URI_PATH: /feature-flags
      UNLEASH_PUBLIC_BASE_URL: https://core-v1.ai4inclusion.org/feature-flags
      TRUST_PROXY: "true"  
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:4242/feature-flags/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: ai4v-kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}
      # Default map if KAFKA_LISTENER_SECURITY_PROTOCOL_MAP is not set:
      # map both PLAINTEXT and PLAINTEXT_INTERNAL to PLAINTEXT.
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "${KAFKA_LISTENER_SECURITY_PROTOCOL_MAP:-PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT}"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    ports:
      - "9093:9092"
    volumes:
      - kafka-data:/var/lib/kafka/data
      - ./infrastructure/kafka:/docker-entrypoint-initdb.d
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 > /dev/null 2>&1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  kong:
    image: kong:3.4-ubuntu
    container_name: ai4v-kong-gateway
    ports:
      - "8000:8000"
      - "8443:8443"
      - "8001:8001"
      - "8444:8444"
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /tmp/kong-substituted.yml
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_ADMIN_GUI_URL: http://localhost:8002
      # Load only our custom plugin to avoid missing bundled plugins issues
      KONG_PLUGINS: token-validator
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      AI4VOICE_API_KEY: ${AI4VOICE_API_KEY}
      ASR_API_KEY: ${ASR_API_KEY}
      TTS_API_KEY: ${TTS_API_KEY}
      NMT_API_KEY: ${NMT_API_KEY}
      PIPELINE_API_KEY: ${PIPELINE_API_KEY}
      DEVELOPER_API_KEY: ${DEVELOPER_API_KEY}
      MODEL_MANAGEMENT_API_KEY: ${MODEL_MANAGEMENT_API_KEY}
      LLM_API_KEY: ${LLM_API_KEY}
    env_file:
      - ./.env
    volumes:
      - ./services/Konga-API-manager/kong-new-architecture.yml:/kong/kong-new-architecture.yml:ro
      - ./services/Konga-API-manager/substitute-env.sh:/kong/substitute-env.sh:ro
      - ./services/Konga-API-manager/plugins/token-validator/kong/plugins:/usr/local/share/lua/5.1/kong/plugins:ro
    command: >
      sh -c "
        sh /kong/substitute-env.sh &&
        /docker-entrypoint.sh kong docker-start
      "
    networks:
      - microservices-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  postgres-konga:
    image: postgres:11-alpine
    container_name: ai4v-postgres-konga
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: konga
    ports:
      - "5435:5432"
    volumes:
      - postgres-konga-data:/var/lib/postgresql/data
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d konga"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped 

  kong-manager:
    image: pantsel/konga:latest
    container_name: ai4v-kong-manager
    ports:
      - "8002:1337"
    environment:
      NODE_ENV: production
      DB_ADAPTER: postgres
      DB_HOST: postgres-konga
      DB_PORT: 5432
      DB_USER: ${POSTGRES_USER}
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_DATABASE: konga
      KONGA_HOOK_TIMEOUT: 120000
      TOKEN_SECRET: ${KONG_MANAGER_TOKEN_SECRET}
    depends_on:
      kong:
        condition: service_healthy
      postgres-konga:
        condition: service_healthy
    networks:
      - microservices-network
    restart: unless-stopped

  # Swagger UI for Kong-facing API documentation
  swagger-kong-ui:
    image: swaggerapi/swagger-ui
    container_name: ai4v-swagger-kong-ui
    ports:
      - "8086:8080"
    volumes:
      # Generated OpenAPI spec that points to Kong and includes all endpoints.
      # Mount it at a custom path; the swagger-ui entrypoint will copy it into
      # /usr/share/nginx/html/openapi.json on startup.
      - ./docs/kong-openapi.json:/kong-openapi.json:ro
    environment:
      SWAGGER_JSON: /kong-openapi.json
    networks:
      - microservices-network
    depends_on:
      - kong

  # Microservices
  api-gateway-service:
    build:
      context: .
      dockerfile: ./services/api-gateway-service/Dockerfile
    container_name: ai4v-api-gateway
    ports:
      - "8080:8080"
    environment:
      - ENV_FILE=.env
      # Swagger/OpenAPI server URL used in the generated docs
      # Prod: point Swagger to public API Gateway URL
      - TRY_IT_LIMIT=5
      - TRY_IT_TTL_SECONDS=3600
    env_file:
      - ./services/api-gateway-service/.env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - microservices-network
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  auth-service:
    build:
      context: .
      dockerfile: ./services/auth-service/Dockerfile
    container_name: ai4v-auth-service
    ports:
      - "8081:8081"
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/auth-service/.env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      microservices-network:
        aliases:
          - auth-service
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  config-service:
    build:
      context: .
      dockerfile: ./services/config-service/Dockerfile
    container_name: ai4v-config-service
    ports:
      - "8082:8082"
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/config-service/.env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
      unleash:
        condition: service_healthy
    networks:
      microservices-network:
        aliases:
          - config-service
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8082/health"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 30s
    restart: unless-stopped

  metrics-service:
    build:
      context: ./services/metrics-service
      dockerfile: Dockerfile
    container_name: ai4v-metrics-service
    ports:
      - "8083:8083"
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/metrics-service/.env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      influxdb:
        condition: service_healthy
    networks:
      - microservices-network
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8083/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  telemetry-service:
    build:
      context: .
      dockerfile: ./services/telemetry-service/Dockerfile
    container_name: ai4v-telemetry-service
    ports:
      - "8084:8084"
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/telemetry-service/.env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - microservices-network
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8084/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  alerting-service:
    build:
      context: .
      dockerfile: ./services/alerting-service/Dockerfile
    container_name: ai4v-alerting-service
    ports:
      - "8085:8085"
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/alerting-service/.env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - microservices-network
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8085/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  dashboard-service:
    build:
      context: ./services/dashboard-service
      dockerfile: Dockerfile
    container_name: ai4v-dashboard-service
    ports:
      - "8090:8086"
      - "8501:8501"  # Streamlit port
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/dashboard-service/.env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      influxdb:
        condition: service_healthy
    networks:
      - microservices-network
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8086/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  asr-service:
    build:
      context: .
      dockerfile: ./services/asr-service/Dockerfile
    container_name: ai4v-asr-service
    ports:
      - "8087:8087"
    environment:
      - ENV_FILE=.env
      - AUTH_ENABLED=false
      - REQUIRE_API_KEY=false
      - ALLOW_ANONYMOUS_ACCESS=true
      - TRITON_ENDPOINT=http://13.200.133.97:9000
      - TRITON_TIMEOUT=120
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      microservices-network:
        aliases:
          - asr-service
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8087/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  tts-service:
    build:
      context: .
      dockerfile: ./services/tts-service/Dockerfile
    container_name: ai4v-tts-service
    ports:
      - "8088:8088"
    environment:
      - ENV_FILE=.env
      - TRITON_ENDPOINT=http://13.200.133.97:9000
    env_file:
      - ./services/tts-service/.env
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      microservices-network:
        aliases:
          - tts-service
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  nmt-service:
    build:
      context: .
      dockerfile: ./services/nmt-service/Dockerfile
    container_name: ai4v-nmt-service
    ports:
      - "8091:8089"
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/nmt-service/.env
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      microservices-network:
        aliases:
          - nmt-service
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8089/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  ocr-service:
    build:
      context: .
      dockerfile: ./services/ocr-service/Dockerfile
    container_name: ai4v-ocr-service
    ports:
      - "8099:8099"
    environment:
      - ENV_FILE=.env
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    dns:
      - 8.8.8.8
      - 8.8.4.4
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8099/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    restart: unless-stopped

  ner-service:
    build:
      context: .
      dockerfile: ./services/ner-service/Dockerfile
    container_name: ai4v-ner-service
    ports:
      - "9001:9001"        # host:container (NER listens on 9001 in the container)
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/ner-service/.env
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      microservices-network:
        aliases:
          - ner-service
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  llm-service:
    build:
      context: .
      dockerfile: ./services/llm-service/Dockerfile
    container_name: ai4v-llm-service
    ports:
      - "8093:8090"
    environment:
      - ENV_FILE=.env
      - TRITON_ENDPOINT=http://3.235.56.233:8001
      - TRITON_TIMEOUT=300
    env_file:
      - ./services/llm-service/.env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      microservices-network:
        aliases:
          - llm-service
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8090/api/v1/llm/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  pipeline-service:
    build:
      context: .
      dockerfile: ./services/pipeline-service/Dockerfile
    container_name: ai4v-pipeline-service
    ports:
      - "8092:8090"
    environment:
      - ENV_FILE=.env
      # Direct service URLs to bypass registry discovery failures
      - ASR_SERVICE_URL=http://asr-service:8087
      - NMT_SERVICE_URL=http://nmt-service:8089
      - TTS_SERVICE_URL=http://tts-service:8088
    env_file:
      - ./services/pipeline-service/.env
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      asr-service:
        condition: service_started
      nmt-service:
        condition: service_started
      tts-service:
        condition: service_started
    networks:
      microservices-network:
        aliases:
          - pipeline-service
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 30s
    restart: unless-stopped

  model-management-service:
    build:
      context: ./services/model-management-service
      dockerfile: Dockerfile
    container_name: ai4v-model-management-service
    ports:
      - "8094:8091"
    environment:
      - APP_DB_USER=${POSTGRES_USER}
      - APP_DB_PASSWORD=${POSTGRES_PASSWORD}
      - APP_DB_HOST=postgres
      - APP_DB_PORT=5432
      - APP_DB_NAME=model_management_db
      - AUTH_DB_USER=${POSTGRES_USER}
      - AUTH_DB_PASSWORD=${POSTGRES_PASSWORD}
      - AUTH_DB_HOST=postgres
      - AUTH_DB_PORT=5432
      - AUTH_DB_NAME=auth_db
      - AUTH_SERVICE_URL=${AUTH_SERVICE_URL}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - RATE_LIMIT_PER_MINUTE=${MODEL_MANAGEMENT_RATE_LIMIT_PER_MINUTE:-60}
      - RATE_LIMIT_PER_HOUR=${MODEL_MANAGEMENT_RATE_LIMIT_PER_HOUR:-1000}
      - AUTH_ENABLED=${MODEL_MANAGEMENT_AUTH_ENABLED:-true}
      - REQUIRE_API_KEY=${MODEL_MANAGEMENT_REQUIRE_API_KEY:-true}
      - ALLOW_ANONYMOUS_ACCESS=${MODEL_MANAGEMENT_ALLOW_ANONYMOUS_ACCESS:-false}
      - MAX_ACTIVE_VERSIONS_PER_MODEL=${MAX_ACTIVE_VERSIONS_PER_MODEL:-5}
      - ALLOW_DEPRECATED_MODEL_CHANGES=${ALLOW_DEPRECATED_MODEL_CHANGES:-true}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      microservices-network:
        aliases:
          - model-management-service
    restart: unless-stopped

  
  multi-tenant-service:
    build:
      context: ./services/multi-tenant-feature
      dockerfile: Dockerfile
    container_name: ai4v-multi-tenant-service
    ports:
      - "8100:8001"
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/multi-tenant-feature/.env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      microservices-network:
        aliases:
          - multi-tenant-service
    restart: unless-stopped

  transliteration-service:
    build:
      context: .
      dockerfile: ./services/transliteration-service/Dockerfile
    container_name: ai4v-transliteration-service
    ports:
      - "8097:8090"
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/transliteration-service/.env
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  language-detection-service:
    build:
      context: .
      dockerfile: ./services/language-detection-service/Dockerfile
    container_name: ai4v-language-detection-service
    ports:
      - "8098:8090"
    environment:
      - ENV_FILE=.env
    env_file:
      - ./services/language-detection-service/.env
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/api/v1/language-detection/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  speaker-diarization-service:
    build:
      context: .
      dockerfile: ./services/speaker-diarization-service/Dockerfile
    container_name: ai4v-speaker-diarization-service
    ports:
      - "8095:8095"
    environment:
      - ENV_FILE=.env
      - TRITON_ENDPOINT=http://65.1.35.3:8700
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8095/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  language-diarization-service:
    build:
      context: .
      dockerfile: ./services/language-diarization-service/Dockerfile
    container_name: ai4v-language-diarization-service
    ports:
      - "9002:8090"
    environment:
      - ENV_FILE=.env
      - TRITON_ENDPOINT=65.1.35.3:8600
    env_file:
      - ./services/language-diarization-service/.env
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  audio-lang-detection-service:
    build:
      context: .
      dockerfile: ./services/audio-lang-detection-service/Dockerfile
    container_name: ai4v-audio-lang-detection-service
    ports:
      - "8096:8096"
    environment:
      - ENV_FILE=.env
      - TRITON_ENDPOINT=65.1.35.3:8100
    env_file:
      - ./services/audio-lang-detection-service/.env
    depends_on:
      config-service:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8096/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped


  simple-ui-frontend:
    build:
      context: ./frontend/simple-ui
      dockerfile: Dockerfile
      args:
        # Browser talks directly to the API Gateway on host port 8080
        NEXT_PUBLIC_API_URL: https://core-v1.ai4inclusion.org
        # Service-specific API keys from .env file (no hardcoding)
        NEXT_PUBLIC_API_KEY: ${AI4VOICE_API_KEY}
        NEXT_PUBLIC_ASR_API_KEY: ${ASR_API_KEY}
        NEXT_PUBLIC_TTS_API_KEY: ${TTS_API_KEY}
        NEXT_PUBLIC_NMT_API_KEY: ${NMT_API_KEY}
        NEXT_PUBLIC_PIPELINE_API_KEY: ${PIPELINE_API_KEY}
        NEXT_PUBLIC_MODEL_MANAGEMENT_API_KEY: ${MODEL_MANAGEMENT_API_KEY}
        # Telemetry/Observability service URL (logs and traces) - same as API Gateway
        NEXT_PUBLIC_TELEMETRY_SERVICE_URL: https://core-v1.ai4inclusion.org
    container_name: ai4v-simple-ui
    ports:
      - "3000:3000"
    environment:
      # HOSTNAME=0.0.0.0 tells Next.js standalone server to listen on all interfaces
      - HOSTNAME=0.0.0.0
      # Route all frontend API calls through the API Gateway on host
      - NEXT_PUBLIC_API_URL=https://core-v1.ai4inclusion.org
      - NEXT_PUBLIC_API_KEY=${AI4VOICE_API_KEY}
      # Service-specific API keys for API Gateway authentication / RBAC
      - NEXT_PUBLIC_ASR_API_KEY=${ASR_API_KEY}
      - NEXT_PUBLIC_TTS_API_KEY=${TTS_API_KEY}
      - NEXT_PUBLIC_NMT_API_KEY=${NMT_API_KEY}
      - NEXT_PUBLIC_PIPELINE_API_KEY=${PIPELINE_API_KEY}
      - NEXT_PUBLIC_MODEL_MANAGEMENT_API_KEY=${MODEL_MANAGEMENT_API_KEY}
      # Telemetry/Observability service URL (logs and traces) - routed through API Gateway
      - NEXT_PUBLIC_TELEMETRY_SERVICE_URL=https://core-v1.ai4inclusion.org
      - NEXT_PUBLIC_ASR_STREAM_URL=ws://asr-service:8087/socket.io/asr
      - NEXT_PUBLIC_TTS_STREAM_URL=ws://tts-service:8088/socket.io/tts
    depends_on:
      api-gateway-service:
        condition: service_healthy
    networks:
      - microservices-network
    # healthcheck:
      # test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      # interval: 30s
      # timeout: 10s
      # retries: 3
      # start_period: 40s
    restart: unless-stopped

  # Observability: Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: ai4v-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./infrastructure/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./infrastructure/prometheus/rules:/etc/prometheus/rules
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.external-url=https://core-v1.ai4inclusion.org/prometheus/'
      # - '--web.external-url=http://localhost:9090'
      - '--web.route-prefix=/'
    networks:
      - microservices-network
    depends_on:
      - nmt-service
      - tts-service
      - asr-service
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Observability: Alertmanager (POC)
  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: ai4v-alertmanager
    ports:
      - "9095:9093"
    volumes:
      - ./infrastructure/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
    networks:
      - microservices-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Alert Configuration Sync Service
  # Generates Prometheus and Alertmanager YAML files from database and triggers hot reload
  alert-config-sync-service:
    build:
      context: ./services/alert-config-sync-service
      dockerfile: Dockerfile
    container_name: ai4v-alert-config-sync-service
    ports:
      - "8101:8097"
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - PROMETHEUS_URL=http://prometheus:9090
      - ALERTMANAGER_URL=http://alertmanager:9093
      - PROMETHEUS_APPLICATION_ALERTS_PATH=/etc/prometheus/rules/application-alerts.yml
      - PROMETHEUS_INFRASTRUCTURE_ALERTS_PATH=/etc/prometheus/rules/infrastructure-alerts.yml
      - ALERTMANAGER_CONFIG_PATH=/etc/alertmanager/alertmanager.yml
      - SYNC_INTERVAL=60
      - PORT=8097
    volumes:
      # Mount Prometheus rules directory (writable so sync service can write YAML files)
      - ./infrastructure/prometheus/rules:/etc/prometheus/rules
      # Mount Alertmanager config file (writable so sync service can write YAML file)
      # Note: alertmanager mounts this as read-only, but sync service writes to host file
      - ./infrastructure/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    depends_on:
      postgres:
        condition: service_healthy
      prometheus:
        condition: service_started
      alertmanager:
        condition: service_started
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8097/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # Observability: Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: ai4v-grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}

      # Combined from both HEAD and remote commits
      - GF_INSTALL_PLUGINS=
      - GF_SERVER_DOMAIN=core-v1.ai4inclusion.org
      - GF_SERVER_ROOT_URL=https://core-v1.ai4inclusion.org/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true

    volumes:
      - grafana-data:/var/lib/grafana
      - ./infrastructure/grafana/provisioning:/etc/grafana/provisioning
    networks:
      - microservices-network
    depends_on:
      - prometheus
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: ai4v-node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--path.rootfs=/rootfs"
    networks:
      - microservices-network


  # Telemetry: OpenSearch (for logs)
  opensearch:
    image: opensearchproject/opensearch:2.11.0
    container_name: ai4v-opensearch
    environment:
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - opensearch-data:/usr/share/opensearch/data
      - ./infrastructure/opensearch/opensearch.yml:/usr/share/opensearch/config/opensearch.yml
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9204:9200"
      - "9600:9600"
    networks:
      - microservices-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # OpenSearch initialization - ensures index template is created on startup
  opensearch-init:
    image: curlimages/curl:latest
    container_name: ai4v-opensearch-init
    depends_on:
      opensearch:
        condition: service_healthy
    networks:
      - microservices-network
    volumes:
      - ./infrastructure/opensearch/init-opensearch.sh:/init-opensearch.sh:ro
      - ./infrastructure/opensearch/index-template.json:/index-template.json:ro
    environment:
      - OPENSEARCH_URL=http://opensearch:9200
      - TEMPLATE_FILE=/index-template.json
    command: ["sh", "/init-opensearch.sh"]
    restart: "no"

  # Telemetry: OpenSearch Dashboards (for log visualization)
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.11.0
    container_name: ai4v-opensearch-dashboards
    ports:
      - "5602:5601"
    environment:
      - OPENSEARCH_HOSTS=http://opensearch:9200
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
      - SERVER_BASEPATH=/opensearch
      - SERVER_REWRITEBASEPATH=true
    networks:
      - microservices-network
    depends_on:
      opensearch:
        condition: service_healthy
    restart: unless-stopped

  # Telemetry: Jaeger (for distributed tracing)
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: ai4v-jaeger
    ports:
      - "16686:16686"  # UI
      - "14268:14268"  # HTTP collector
      - "6831:6831/udp"  # UDP agent
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - QUERY_BASE_PATH=/jaeger
    networks:
      - microservices-network
    restart: unless-stopped

  # Telemetry: Fluent Bit (for log collection)
  fluent-bit:
    image: fluent/fluent-bit:latest
    container_name: ai4v-fluent-bit
    volumes:
      - ./infrastructure/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf
      - ./infrastructure/fluent-bit/parsers.conf:/fluent-bit/etc/parsers.conf
      - ./infrastructure/fluent-bit/extract_context.lua:/fluent-bit/scripts/extract_context.lua:ro
      # - /var/snap/docker/common/var-lib-docker/containers:/var/lib/docker/containers:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - microservices-network
    depends_on:
      opensearch:
        condition: service_healthy
    restart: unless-stopped

# Policy Engine
  policy-engine:
    build:
      context: ./services/policy-engine
      dockerfile: Dockerfile
    container_name: ai4v-policy-engine
    ports:
      - "8102:8095"
    environment:
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    networks:
      - microservices-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8095/health"]
      interval: 30s
      timeout: 5s


volumes:
  postgres-data:
    driver: local
    # Note: Removed bind mount to Linux-specific path for Windows compatibility
    # Data will persist in Docker's managed volume location
    # To use a custom path on Linux, uncomment and adjust the driver_opts below:
    driver_opts:
      type: "none"
      o: "bind"
      device: "/home/ubuntu/ai4i-v/volumes/pg_data"
      
  postgres-konga-data:
    driver: local
  redis-data:
    driver: local
  influxdb-data:
    driver: local
  elasticsearch-data:
    driver: local
  kafka-data:
    driver: local
  zookeeper-data:
    driver: local
  zookeeper-logs:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  opensearch-data:
    driver: local
  alertmanager-data:
    driver: local

networks:
  microservices-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.31.0.0/16
